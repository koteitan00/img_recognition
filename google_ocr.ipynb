{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41c22aa-d628-4037-8f16-e68d16e05e54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1c2e17-ea96-45fe-b65e-fb272c1f2851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd46a61e-888d-40f4-ae2a-1e6458c4455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T14:53:53.138703Z",
     "start_time": "2023-05-23T14:53:53.097171Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992b6658-efd2-41b3-a71f-3f20fb8b97ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T14:53:53.138703Z",
     "start_time": "2023-05-23T14:53:53.097171Z"
    }
   },
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "MIME_TYPE = 'application/vnd.google-apps.document'\n",
    "APPLICATION_NAME = 'ipa-google-drive-api-client'\n",
    "\n",
    "def get_service():\n",
    "\n",
    "    # credentialの取得\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'google-drive-api.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    # serviceの取得\n",
    "    service = build('drive', 'v3', credentials=creds) \n",
    "    \n",
    "    return service\n",
    "\n",
    "def read_ocr(service, input_file, lang='jp'):\n",
    "    # ファイルのアップロード\n",
    "\n",
    "    # ローカルファイルの定義\n",
    "    media_body = MediaFileUpload(input_file, mimetype=MIME_TYPE, resumable=True)\n",
    "\n",
    "    # Google Drive上のファイル名\n",
    "    newfile = 'output.pdf'\n",
    "\n",
    "    body = {\n",
    "        'name': newfile,\n",
    "        'mimeType': MIME_TYPE\n",
    "    }\n",
    "\n",
    "    # 　creat関数でファイルアップロード実行\n",
    "    # 同時にOCR読み取りも行う\n",
    "    output = service.files().create(\n",
    "        body=body,\n",
    "        media_body=media_body,\n",
    "        # ここで読み込み先言語の指定を行う\n",
    "        ocrLanguage=lang,\n",
    "    ).execute()\n",
    "\n",
    "    # テキストファイルのダウンロード\n",
    "\n",
    "    # リクエストオブジェクト生成\n",
    "    request = service.files().export_media(\n",
    "        fileId=output['id'],\n",
    "        mimeType=\"text/plain\"\n",
    "    )\n",
    "    output_path = 'output.txt'\n",
    "\n",
    "    with open(output_path, 'a') as f:\n",
    "        fh = io.FileIO(output_path, \"wb\")\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "\n",
    "        service.files().delete(fileId=output['id']).execute()\n",
    "    \n",
    "        # テキストの取得\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 読み取り結果のリストを返す\n",
    "    return lines[1:]\n",
    "\n",
    "\n",
    "service = get_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94045e0-35ba-4a5d-8254-d9a671c7339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(text_file):\n",
    "    output_dir = \"C:/Users/covid/text_recognition/output\"\n",
    "    if os.path.exists(output_dir):\n",
    "        file_list = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "            os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        lines = sorted(lines, key=lambda line: float(line.split()[1]))\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            values = line.split()\n",
    "\n",
    "            if len(values) == 5:\n",
    "                object_class = values[0]\n",
    "                a = float(values[1])\n",
    "                b = float(values[2])\n",
    "                c = float(values[3])\n",
    "                d = float(values[4])\n",
    "\n",
    "                # Calculate coordinates and dimensions\n",
    "                x_center = int(wid * a)\n",
    "                y_center = int(hei * b)\n",
    "                width = int(wid * c)\n",
    "                height = int(hei * d)\n",
    "\n",
    "                x_min = x_center - width // 2\n",
    "                y_min = y_center - height // 2\n",
    "                x_max = x_center + width // 2\n",
    "                y_max = y_center + height // 2\n",
    "\n",
    "                output_filename = os.path.join(output_dir, f'book{i+1}.jpg')\n",
    "                index = 1\n",
    "                while os.path.exists(output_filename):\n",
    "                    output_filename = os.path.join(output_dir, f'book{i+1}_{index}.jpg')\n",
    "                    index += 1\n",
    "\n",
    "                # Crop and save the image\n",
    "                cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "                cropped.save(output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde91463-c141-461e-9d3d-af02f4c1a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable list\n",
    "# ディレクトリのパス\n",
    "directory_path = \"C:/Users/covid/text_recognition/yolov7/runs/detect/\"\n",
    "# 画像ファイルの相対パスを指定\n",
    "image_relative_path = \"input.png\"\n",
    "# テキストファイルの相対パスを指定\n",
    "text_file_relative_path = \"labels/input.txt\"\n",
    "\n",
    "out_path = 'C:/Users/covid/text_recognition/output'\n",
    "output_file = \"C:/Users/covid/text_recognition/output_results.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbdca0-abe2-41e8-ab29-9c0d0a553d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9ce244-8ea8-462f-b9a1-97f0c7b7b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "cv2.imwrite(\"C:/Users/covid/text_recognition/yolov7/input.png\",frame)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd81928-34a6-4223-9e45-7c8283efac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\\yolov7\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\\yolov7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47c2ccd2-7059-4b24-967a-3b588652eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3504fcd-93e7-427a-944d-161d4f3b56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7-e6e.pt'], source='C:/Users/covid/text_recognition/yolov7/input.png', img_size=1280, conf_thres=0.25, iou_thres=0.45, device='0', view_img=False, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "16 books, Done. (42.0ms) Inference, (47.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp21\\input.png\n",
      "Done. (0.523s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  v0.1-126-g84932d7 torch 2.1.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3090, 24575.5MB)\n",
      "\n",
      "Model Summary: 792 layers, 151687420 parameters, 817020 gradients\n",
      "C:\\Users\\covid\\anaconda3\\envs\\localGPU\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source C:/Users/covid/text_recognition/yolov7/input.png --weights yolov7-e6e.pt --conf 0.25 --img-size 1280 --device 0 --save-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1820bb9-6774-4ac4-8085-ab3c7f8ec3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)\n",
    "\n",
    "# 新しいディレクトリに移動\n",
    "os.chdir(newest_subdirectory_path)\n",
    "\n",
    "# 画像ファイルの絶対パスを作成\n",
    "image_absolute_path = os.path.join(newest_subdirectory_path, image_relative_path)\n",
    "# テキストファイルの絶対パスを作成\n",
    "text_file_absolute_path = os.path.join(newest_subdirectory_path, text_file_relative_path)\n",
    "\n",
    "# 画像をImageクラスのインスタンスに読み込む\n",
    "image = Image.open(image_absolute_path)\n",
    "# テキストファイルを読み込む\n",
    "with open(text_file_absolute_path, 'r') as file:\n",
    "    text_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e30fd2d9-71f2-47b1-a602-2b1d9b7c14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text file\n",
    "wid,hei = image.size\n",
    "process_text_file(text_file_absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999c5e72-1c01-446e-bec0-a6bc50775a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output corresponding to list format\n",
    "if __name__ == '__main__':\n",
    "    output_list = []\n",
    "\n",
    "    file_list = [filename for filename in os.listdir(out_path) if filename.endswith('.jpg')]\n",
    "    file_list.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    for filename in file_list:\n",
    "        input_file = os.path.join(out_path, filename)\n",
    "        output = read_ocr(service, input_file, 'ja')\n",
    "\n",
    "        # 不要な文字（スペースとバックスラッシュ）を除去して一つの文字列に結合する\n",
    "        cleaned_output = ''.join(line.strip().replace(' ', '').replace('/', '').replace('\\n', '').replace('\\\\', '') for line in output)\n",
    "\n",
    "        # 結果をリストに追加\n",
    "        output_list.append(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e40376-a25b-4581-beb2-9e987664e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to C:/Users/covid/text_recognition/output_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the results to the output file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for result in output_list:\n",
    "            file.write(result + '\\n')\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8433c-74fa-4409-9410-da659b44c5d4",
   "metadata": {},
   "source": [
    "## Compare Methods of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e791e-ffb3-4710-89d3-d281d8017ba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Different.SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b25da00-05b8-4cb9-9cbd-fd60b655e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\\\Users\\\\covid\\\\text_recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "701f522b-4abb-4bd9-a9d1-53415a4b2590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match in Text B (line 4): DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest Similarity Ratio: 0.3548\n",
      "\n",
      "Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match in Text B (line 5): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.7442\n",
      "\n",
      "Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match in Text B (line 5): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.6667\n",
      "\n",
      "Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match in Text B (line 5): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.4681\n",
      "\n",
      "Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match in Text B (line 8): クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest Similarity Ratio: 0.6939\n",
      "\n",
      "Text A (line 6): 門ベイズ充十\n",
      "Best Match in Text B (line 10): 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest Similarity Ratio: 0.3030\n",
      "\n",
      "Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match in Text B (line 11): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Similarity Ratio: 0.5965\n",
      "\n",
      "Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match in Text B (line 11): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Similarity Ratio: 0.3636\n",
      "\n",
      "Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match in Text B (line 13): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Similarity Ratio: 0.4211\n",
      "\n",
      "Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match in Text B (line 14): 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest Similarity Ratio: 0.4561\n",
      "\n",
      "Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match in Text B (line 15): 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest Similarity Ratio: 0.5405\n",
      "\n",
      "Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match in Text B (line 16): Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest Similarity Ratio: 0.9375\n",
      "\n",
      "Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match in Text B (line 17): 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Similarity Ratio: 0.6506\n",
      "\n",
      "Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match in Text B (line 18): デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Similarity Ratio: 0.7436\n",
      "\n",
      "Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match in Text B (line 19): 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Similarity Ratio: 0.5143\n",
      "\n",
      "Text A (line 16): 大学\n",
      "Best Match in Text B (line 13): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Similarity Ratio: 0.2308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "        best_match_text_b = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = difflib.SequenceMatcher(None, text_a, text_b).ratio()\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "                best_match_text_b = text_b\n",
    "\n",
    "        print(f\"Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match in Text B (line {lines_b.index(best_match) + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e727c425-5ef0-4898-a436-836db1771d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"output_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = difflib.SequenceMatcher(None, text_a, text_b).ratio()\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Similarity Ratio: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7afaa8-03cb-41ba-9122-46a2d0c15d05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b026ed89-2870-4469-9be3-05b8818d9871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match for Text A (line 1): DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest Similarity Ratio: 0.3333\n",
      "\n",
      "Original Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match for Text A (line 2): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.7317\n",
      "\n",
      "Original Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match for Text A (line 3): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.6522\n",
      "\n",
      "Original Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match for Text A (line 4): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Similarity Ratio: 0.4444\n",
      "\n",
      "Original Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match for Text A (line 5): クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest Similarity Ratio: 0.6809\n",
      "\n",
      "Original Text A (line 6): 門ベイズ充十\n",
      "Best Match for Text A (line 6): 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest Similarity Ratio: 0.2581\n",
      "\n",
      "Original Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match for Text A (line 7): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Similarity Ratio: 0.5818\n",
      "\n",
      "Original Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match for Text A (line 8): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Similarity Ratio: 0.3396\n",
      "\n",
      "Original Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match for Text A (line 9): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Similarity Ratio: 0.3889\n",
      "\n",
      "Original Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match for Text A (line 10): 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest Similarity Ratio: 0.4364\n",
      "\n",
      "Original Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match for Text A (line 11): 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest Similarity Ratio: 0.5143\n",
      "\n",
      "Original Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match for Text A (line 12): Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest Similarity Ratio: 0.9367\n",
      "\n",
      "Original Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match for Text A (line 13): 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Similarity Ratio: 0.6420\n",
      "\n",
      "Original Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match for Text A (line 14): デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Similarity Ratio: 0.7368\n",
      "\n",
      "Original Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match for Text A (line 15): 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Similarity Ratio: 0.4848\n",
      "\n",
      "Original Text A (line 16): 大学\n",
      "Best Match for Text A (line 16): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Similarity Ratio: 0.1667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = Levenshtein.ratio(text_a.strip(), text_b.strip())\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "\n",
    "        print(f\"Original Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9b1a02a-d969-41cc-9259-99c68cd0197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"levenshtein_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = Levenshtein.ratio(text_a.strip(), text_b.strip())\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Similarity Ratio: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = text_b\n",
    "\n",
    "            output.write(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\\n\")\n",
    "            output.write(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10ac69-6023-489f-b011-da3a9cdf0fbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cec8f9cc-613e-4429-9362-5e3806445fee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match for Text A (line 1): DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest Jaccard Coefficient: 0.2439\n",
      "\n",
      "Original Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match for Text A (line 2): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Jaccard Coefficient: 0.6522\n",
      "\n",
      "Original Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match for Text A (line 3): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Jaccard Coefficient: 0.6000\n",
      "\n",
      "Original Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match for Text A (line 4): キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Jaccard Coefficient: 0.3125\n",
      "\n",
      "Original Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match for Text A (line 5): クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest Jaccard Coefficient: 0.5926\n",
      "\n",
      "Original Text A (line 6): 門ベイズ充十\n",
      "Best Match for Text A (line 6): 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest Jaccard Coefficient: 0.1481\n",
      "\n",
      "Original Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match for Text A (line 7): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Jaccard Coefficient: 0.4688\n",
      "\n",
      "Original Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match for Text A (line 8): やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Jaccard Coefficient: 0.2368\n",
      "\n",
      "Original Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match for Text A (line 9): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Jaccard Coefficient: 0.2593\n",
      "\n",
      "Original Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match for Text A (line 10): 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest Jaccard Coefficient: 0.2821\n",
      "\n",
      "Original Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match for Text A (line 11): 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest Jaccard Coefficient: 0.3750\n",
      "\n",
      "Original Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match for Text A (line 12): Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest Jaccard Coefficient: 0.9583\n",
      "\n",
      "Original Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match for Text A (line 13): 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Jaccard Coefficient: 0.5333\n",
      "\n",
      "Original Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match for Text A (line 14): デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Jaccard Coefficient: 0.5814\n",
      "\n",
      "Original Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match for Text A (line 15): 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Jaccard Coefficient: 0.3200\n",
      "\n",
      "Original Text A (line 16): 大学\n",
      "Best Match for Text A (line 16): 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Jaccard Coefficient: 0.1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def jaccard_coefficient(s1, s2):\n",
    "    set1 = set(s1)\n",
    "    set2 = set(s2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = jaccard_coefficient(text_a.strip(), text_b.strip())\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "\n",
    "        print(f\"Original Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Jaccard Coefficient: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "416f1222-e91f-434b-bea1-228250c67966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(s1, s2):\n",
    "    set1 = set(s1)\n",
    "    set2 = set(s2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"jaccard_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = jaccard_coefficient(text_a.strip(), text_b.strip())\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Jaccard Coefficient: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "351d00e9-c464-4b32-b490-7686a99c3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_cosine_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"cosine_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, line_b in enumerate(lines_b):\n",
    "                similarity = cosine_sim[i][j]\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {line_b.strip()}\\n\")\n",
    "                output.write(f\"Cosine Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f35f20-4530-4ddb-a9b7-9f8c29dd5957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efa820ea-4667-433a-b157-d3bb87a21d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全文出力\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_ngram_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_ngram_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"ngram_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"N-gram Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d33791a5-ee62-4a44-a4d8-421ffc03030d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match for Text A: DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest N-gram Similarity: 0.2236\n",
      "\n",
      "Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest N-gram Similarity: 0.5380\n",
      "\n",
      "Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest N-gram Similarity: 0.5839\n",
      "\n",
      "Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest N-gram Similarity: 0.3722\n",
      "\n",
      "Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match for Text A: クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest N-gram Similarity: 0.5657\n",
      "\n",
      "Text A (line 6): 門ベイズ充十\n",
      "Best Match for Text A: 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest N-gram Similarity: 0.1826\n",
      "\n",
      "Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest N-gram Similarity: 0.4264\n",
      "\n",
      "Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest N-gram Similarity: 0.2335\n",
      "\n",
      "Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match for Text A: 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest N-gram Similarity: 0.0605\n",
      "\n",
      "Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match for Text A: 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest N-gram Similarity: 0.1557\n",
      "\n",
      "Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match for Text A: 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest N-gram Similarity: 0.3150\n",
      "\n",
      "Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match for Text A: Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest N-gram Similarity: 0.6530\n",
      "\n",
      "Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest N-gram Similarity: 0.4499\n",
      "\n",
      "Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest N-gram Similarity: 0.6966\n",
      "\n",
      "Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest N-gram Similarity: 0.3974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#最大値出力\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_ngram_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_ngram_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"ngram_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest N-gram Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest N-gram Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ec326-d1f4-45eb-88fa-1766cdfa17c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 部分文字列比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa8ac942-a15b-4e94-a266-f7002e8ed523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_partial_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_partial_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"partial_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"Partial Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8c962b2-846e-4892-b4a3-d9b247cf6e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match for Text A: DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest Partial Similarity: 0.2236\n",
      "\n",
      "Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Partial Similarity: 0.5380\n",
      "\n",
      "Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Partial Similarity: 0.5839\n",
      "\n",
      "Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Partial Similarity: 0.3722\n",
      "\n",
      "Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match for Text A: クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest Partial Similarity: 0.5657\n",
      "\n",
      "Text A (line 6): 門ベイズ充十\n",
      "Best Match for Text A: 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest Partial Similarity: 0.1826\n",
      "\n",
      "Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Partial Similarity: 0.4264\n",
      "\n",
      "Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Partial Similarity: 0.2335\n",
      "\n",
      "Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match for Text A: 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Partial Similarity: 0.0605\n",
      "\n",
      "Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match for Text A: 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest Partial Similarity: 0.1557\n",
      "\n",
      "Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match for Text A: 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest Partial Similarity: 0.3150\n",
      "\n",
      "Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match for Text A: Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest Partial Similarity: 0.6530\n",
      "\n",
      "Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Partial Similarity: 0.4499\n",
      "\n",
      "Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Partial Similarity: 0.6966\n",
      "\n",
      "Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Partial Similarity: 0.3974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_partial_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_partial_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"partial_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest Partial Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest Partial Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54edd8f-1b93-4940-9d31-fc4f5efcd12d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ローマッチング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77f27daa-d48a-4e6b-8dbb-ac1d33133626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def calculate_levenshtein_distance(texts):\n",
    "    distance_matrix = [[0 for _ in range(len(texts[1]))] for _ in range(len(texts[0]))]\n",
    "    for i in range(len(texts[0])):\n",
    "        for j in range(len(texts[1])):\n",
    "            distance_matrix[i][j] = Levenshtein.distance(texts[0][i], texts[1][j])\n",
    "    return distance_matrix\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    levenshtein_distances = calculate_levenshtein_distance([lines_a, lines_b])\n",
    "\n",
    "    output_file = \"levenshtein_distance_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, distance in enumerate(levenshtein_distances[i]):\n",
    "                similarity = 1 - (distance / max(len(lines_a[i]), len(lines_b[j])))\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"Levenshtein Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "538f1bff-c0eb-47e7-bbd4-76e38cd92d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 力学系E四ツ谷二C・ロビンソン常讀書店\n",
      "Best Match for Text A: DYNAMICALSYSTEMS力学系上C・ロビンソン著國府寛司・柴山健伸岡宏枝訳\n",
      "Highest Levenshtein Similarity: 0.2143\n",
      "\n",
      "Text A (line 2): キーポイント線形代数四ツ谷吉四ツ谷二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Levenshtein Similarity: 0.6667\n",
      "\n",
      "Text A (line 3): キーポイント線形代数キーポイントで攻四ツ谷昌二\n",
      "Best Match for Text A: キーポイント線形代数薩摩順吉四ツ谷昌二岩波書店\n",
      "Highest Levenshtein Similarity: 0.5000\n",
      "\n",
      "Text A (line 4): と化SEGAITAINEキーポイント線形代数\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Levenshtein Similarity: 0.1042\n",
      "\n",
      "Text A (line 5): シリーズ数理統計学の基礎尾畑伸明\n",
      "Best Match for Text A: クロスセクショナル統計シリーズ1数理統計学の基礎尾畑伸明[著]\n",
      "Highest Levenshtein Similarity: 0.5312\n",
      "\n",
      "Text A (line 6): 門ベイズ充十\n",
      "Best Match for Text A: 入門ベイズ統計意思決定の理論と発展松原望著東京図書\n",
      "Highest Levenshtein Similarity: 0.1923\n",
      "\n",
      "Text A (line 7): さしく知りたい入門ベイズ充十ベイズ統計学松原里\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Levenshtein Similarity: 0.4848\n",
      "\n",
      "Text A (line 8): ALやさしく知りたい全生命改訂2版医学出版\n",
      "Best Match for Text A: やさしく知りたい先端科学シリーズ1ベイズ統計学松原望創元社創元社\n",
      "Highest Levenshtein Similarity: 0.2424\n",
      "\n",
      "Text A (line 9): 爆發,線型代数人門齋藤正彦苦\n",
      "Best Match for Text A: 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Levenshtein Similarity: 0.3478\n",
      "\n",
      "Text A (line 10): 解析演習杉源无夫·清水芙男全予芜·国本和夫\n",
      "Best Match for Text A: 数学基礎7解析演習杉浦光夫・清水英男金子晃・岡本和夫著東京大学出版会\n",
      "Highest Levenshtein Similarity: 0.3714\n",
      "\n",
      "Text A (line 11): 一中,解析入門,杉浦光夫著\n",
      "Best Match for Text A: 数学基礎2解析入門Ⅰ杉浦光夫著東京大学出版会\n",
      "Highest Levenshtein Similarity: 0.4348\n",
      "\n",
      "Text A (line 12): StructureandInterpretationofComputerProgramsSecondEditionAbelsonandSussman\n",
      "Best Match for Text A: Structure and Interpretation of Computer Programs Second Edition Abelson and Sussman\n",
      "Highest Levenshtein Similarity: 0.8824\n",
      "\n",
      "Text A (line 13): 微生物の力学系竹内ハル・スミスポール・ウォルトマン(著日本\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Levenshtein Similarity: 0.5094\n",
      "\n",
      "Text A (line 14): デクステリティ巧みさとその発達ニコライ・ベルンシュタインK\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Levenshtein Similarity: 0.6042\n",
      "\n",
      "Text A (line 15): 理科系の作文技術阅各質之O\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Levenshtein Similarity: 0.4286\n",
      "\n",
      "Text A (line 16): 大学\n",
      "Best Match for Text A: 数学基礎1線形代数入門斎藤正彦東京大学出版会\n",
      "Highest Levenshtein Similarity: 0.1304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def calculate_levenshtein_distance(texts):\n",
    "    distance_matrix = [[0 for _ in range(len(texts[1]))] for _ in range(len(texts[0]))]\n",
    "    for i in range(len(texts[0])):\n",
    "        for j in range(len(texts[1])):\n",
    "            distance_matrix[i][j] = Levenshtein.distance(texts[0][i], texts[1][j])\n",
    "    return distance_matrix\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    levenshtein_distances = calculate_levenshtein_distance([lines_a, lines_b])\n",
    "\n",
    "    output_file = \"levenshtein_distance_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, distance in enumerate(levenshtein_distances[i]):\n",
    "                similarity = 1 - (distance / max(len(lines_a[i]), len(lines_b[j])))\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest Levenshtein Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest Levenshtein Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "localGPU",
   "language": "python",
   "name": "localgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
