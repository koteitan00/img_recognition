{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41c22aa-d628-4037-8f16-e68d16e05e54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1c2e17-ea96-45fe-b65e-fb272c1f2851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd46a61e-888d-40f4-ae2a-1e6458c4455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T14:53:53.138703Z",
     "start_time": "2023-05-23T14:53:53.097171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992b6658-efd2-41b3-a71f-3f20fb8b97ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T14:53:53.138703Z",
     "start_time": "2023-05-23T14:53:53.097171Z"
    }
   },
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "MIME_TYPE = 'application/vnd.google-apps.document'\n",
    "APPLICATION_NAME = 'ipa-google-drive-api-client'\n",
    "\n",
    "def get_service():\n",
    "\n",
    "    # credentialの取得\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'google-drive-api.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    # serviceの取得\n",
    "    service = build('drive', 'v3', credentials=creds) \n",
    "    \n",
    "    return service\n",
    "\n",
    "def read_ocr(service, input_file, lang='jp'):\n",
    "    # ファイルのアップロード\n",
    "\n",
    "    # ローカルファイルの定義\n",
    "    media_body = MediaFileUpload(input_file, mimetype=MIME_TYPE, resumable=True)\n",
    "\n",
    "    # Google Drive上のファイル名\n",
    "    newfile = 'output.pdf'\n",
    "\n",
    "    body = {\n",
    "        'name': newfile,\n",
    "        'mimeType': MIME_TYPE\n",
    "    }\n",
    "\n",
    "    # 　creat関数でファイルアップロード実行\n",
    "    # 同時にOCR読み取りも行う\n",
    "    output = service.files().create(\n",
    "        body=body,\n",
    "        media_body=media_body,\n",
    "        # ここで読み込み先言語の指定を行う\n",
    "        ocrLanguage=lang,\n",
    "    ).execute()\n",
    "\n",
    "    # テキストファイルのダウンロード\n",
    "\n",
    "    # リクエストオブジェクト生成\n",
    "    request = service.files().export_media(\n",
    "        fileId=output['id'],\n",
    "        mimeType=\"text/plain\"\n",
    "    )\n",
    "    output_path = 'output.txt'\n",
    "\n",
    "    with open(output_path, 'a') as f:\n",
    "        fh = io.FileIO(output_path, \"wb\")\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "\n",
    "        service.files().delete(fileId=output['id']).execute()\n",
    "    \n",
    "        # テキストの取得\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 読み取り結果のリストを返す\n",
    "    return lines[1:]\n",
    "\n",
    "\n",
    "service = get_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94045e0-35ba-4a5d-8254-d9a671c7339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(text_file):\n",
    "    output_dir = \"C:/Users/covid/text_recognition/output\"\n",
    "    if os.path.exists(output_dir):\n",
    "        file_list = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "            os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        lines = sorted(lines, key=lambda line: float(line.split()[1]))\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            values = line.split()\n",
    "\n",
    "            if len(values) == 5:\n",
    "                object_class = values[0]\n",
    "                a = float(values[1])\n",
    "                b = float(values[2])\n",
    "                c = float(values[3])\n",
    "                d = float(values[4])\n",
    "\n",
    "                # Calculate coordinates and dimensions\n",
    "                x_center = int(wid * a)\n",
    "                y_center = int(hei * b)\n",
    "                width = int(wid * c)\n",
    "                height = int(hei * d)\n",
    "\n",
    "                x_min = x_center - width // 2\n",
    "                y_min = y_center - height // 2\n",
    "                x_max = x_center + width // 2\n",
    "                y_max = y_center + height // 2\n",
    "\n",
    "                output_filename = os.path.join(output_dir, f'book{i+1}.jpg')\n",
    "                index = 1\n",
    "                while os.path.exists(output_filename):\n",
    "                    output_filename = os.path.join(output_dir, f'book{i+1}_{index}.jpg')\n",
    "                    index += 1\n",
    "\n",
    "                # Crop and save the image\n",
    "                cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "                cropped.save(output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0464a813-edff-46e4-bc2f-c42e3b828af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カメラの読込み\n",
    "# 内蔵カメラがある場合、下記引数の数字を変更する必要あり\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 動画終了まで、1フレームずつ読み込んで表示する。\n",
    "while(cap.isOpened()):\n",
    "    # 1フレーム毎　読込み\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # GUIに表示\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    # qキーが押されたら途中終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 終了処理\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bde91463-c141-461e-9d3d-af02f4c1a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable list\n",
    "# ディレクトリのパス\n",
    "directory_path = \"C:/Users/covid/text_recognition/yolov7/runs/detect/\"\n",
    "# 画像ファイルの相対パスを指定\n",
    "image_relative_path = \"input.png\"\n",
    "# テキストファイルの相対パスを指定\n",
    "text_file_relative_path = \"labels/input.txt\"\n",
    "\n",
    "out_path = 'C:/Users/covid/text_recognition/output'\n",
    "output_file = \"C:/Users/covid/text_recognition/output_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bf8b8d54-f2f9-4d9d-b344-002b96254b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if ret:#カメラと本棚の距離→0.6m\n",
    "    x = 0  # トリミングの左上のX座標\n",
    "    y = 90  # トリミングの左上のY座標\n",
    "    width = 640  # トリミングする幅\n",
    "    height = 200  # トリミングする高さ\n",
    "    \n",
    "    cropped_frame = frame[y:y+height, x:x+width]\n",
    "\n",
    "    cv2.imwrite(\"C:/Users/covid/text_recognition/yolov7/input.png\", cropped_frame)\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbdca0-abe2-41e8-ab29-9c0d0a553d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b9ce244-8ea8-462f-b9a1-97f0c7b7b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "cv2.imwrite(\"C:/Users/covid/text_recognition/yolov7/input.png\",frame)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddd81928-34a6-4223-9e45-7c8283efac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\\yolov7\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\\yolov7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47c2ccd2-7059-4b24-967a-3b588652eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3504fcd-93e7-427a-944d-161d4f3b56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7-e6e.pt'], source='C:/Users/covid/text_recognition/yolov7/input.png', img_size=1280, conf_thres=0.25, iou_thres=0.45, device='0', view_img=False, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "19 books, Done. (27.0ms) Inference, (43.0ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp16\\input.png\n",
      "Done. (0.612s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  v0.1-126-g84932d7 torch 2.1.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3090, 24575.5MB)\n",
      "\n",
      "Model Summary: 792 layers, 151687420 parameters, 817020 gradients\n",
      "C:\\Users\\covid\\anaconda3\\envs\\localGPU\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source C:/Users/covid/text_recognition/yolov7/input.png --weights yolov7-e6e.pt --conf 0.25 --img-size 1280 --device 0 --save-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1820bb9-6774-4ac4-8085-ab3c7f8ec3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)\n",
    "\n",
    "# 新しいディレクトリに移動\n",
    "os.chdir(newest_subdirectory_path)\n",
    "\n",
    "# 画像ファイルの絶対パスを作成\n",
    "image_absolute_path = os.path.join(newest_subdirectory_path, image_relative_path)\n",
    "# テキストファイルの絶対パスを作成\n",
    "text_file_absolute_path = os.path.join(newest_subdirectory_path, text_file_relative_path)\n",
    "\n",
    "# 画像をImageクラスのインスタンスに読み込む\n",
    "image = Image.open(image_absolute_path)\n",
    "# テキストファイルを読み込む\n",
    "with open(text_file_absolute_path, 'r') as file:\n",
    "    text_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e30fd2d9-71f2-47b1-a602-2b1d9b7c14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text file\n",
    "wid,hei = image.size\n",
    "process_text_file(text_file_absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c5e72-1c01-446e-bec0-a6bc50775a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output corresponding to list format\n",
    "if __name__ == '__main__':\n",
    "    output_list = []\n",
    "\n",
    "    file_list = [filename for filename in os.listdir(out_path) if filename.endswith('.jpg')]\n",
    "    file_list.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    for filename in file_list:\n",
    "        input_file = os.path.join(out_path, filename)\n",
    "        output = read_ocr(service, input_file, 'ja')\n",
    "\n",
    "        # 不要な文字（スペースとバックスラッシュ）を除去して一つの文字列に結合する\n",
    "        cleaned_output = ''.join(line.strip().replace(' ', '').replace('/', '').replace('\\n', '').replace('\\\\', '') for line in output)\n",
    "\n",
    "        # 結果をリストに追加\n",
    "        output_list.append(cleaned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4e40376-a25b-4581-beb2-9e987664e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to C:/Users/covid/text_recognition/output_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the results to the output file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for result in output_list:\n",
    "            file.write(result + '\\n')\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8433c-74fa-4409-9410-da659b44c5d4",
   "metadata": {},
   "source": [
    "## Compare Methods of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e791e-ffb3-4710-89d3-d281d8017ba6",
   "metadata": {},
   "source": [
    "### Different.SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "701f522b-4abb-4bd9-a9d1-53415a4b2590",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 機械振動学湖著\n",
      "Best Match in Text B (line 1): 機械振動学佐藤秀紀岡部佐規一共著岩田佳雄元工業調査会\n",
      "Highest Similarity Ratio: 0.4000\n",
      "\n",
      "Text A (line 2): ロボット制御から動力学までロボット制御基礎から動力学まで浅ロボット制御\n",
      "Best Match in Text B (line 2): 実践ロボット制御基礎から動力学まで細田耕[著]HosodaKoh株式会社アールティ[協力]RTCorporationOhmsha\n",
      "Highest Similarity Ratio: 0.3168\n",
      "\n",
      "Text A (line 3): book情報理論と符号理論Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEchoGoogleHuneジョーンズポンダッドw.マイナビ\n",
      "Best Match in Text B (line 5): Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest Similarity Ratio: 0.8028\n",
      "\n",
      "Text A (line 4): P情報理論と符号理論ジョーンズTMジョーンズ\n",
      "Best Match in Text B (line 6): 情報理論と符合理論G・A・ジョーンズ著J・M・ジョーンズ一樂重雄/川原正治/川原雅子訳丸善\n",
      "Highest Similarity Ratio: 0.5797\n",
      "\n",
      "Text A (line 5): 29パターン認識と機械学習上CM・ビショップベイズによる\n",
      "Best Match in Text B (line 8): パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Similarity Ratio: 0.4773\n",
      "\n",
      "Text A (line 6): CM・ビショップパターン認識と機械学習1:3\n",
      "Best Match in Text B (line 7): はじめてのパターン認識平井有三著8497\n",
      "Highest Similarity Ratio: 0.3182\n",
      "\n",
      "Text A (line 7): パターン認識と機械学習による\n",
      "Best Match in Text B (line 8): パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Similarity Ratio: 0.4054\n",
      "\n",
      "Text A (line 8): マルコフ連鎖から格子確率モデルへしCM・ビショップ\n",
      "Best Match in Text B (line 11): マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Similarity Ratio: 0.5429\n",
      "\n",
      "Text A (line 9): マルコフ連鎖から格子確率モデルへ\n",
      "Best Match in Text B (line 11): マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Similarity Ratio: 0.5574\n",
      "\n",
      "Text A (line 10): スパース性に基づく機械学習新同亮太上\n",
      "Best Match in Text B (line 12): 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest Similarity Ratio: 0.5424\n",
      "\n",
      "Text A (line 11): モジュラ化\n",
      "Best Match in Text B (line 13): MLPP機械学習シリーズプロフェッショナル劣モジュラ最適化と機械学習河原吉伸永野清仁講談社講談社\n",
      "Highest Similarity Ratio: 0.2182\n",
      "\n",
      "Text A (line 12): a\n",
      "Best Match in Text B (line 7): はじめてのパターン認識平井有三著8497\n",
      "Highest Similarity Ratio: 0.0870\n",
      "\n",
      "Text A (line 13): オペレーションズリサーチ\n",
      "Best Match in Text B (line 15): 情報・技術経営シリーズ4経営情報処理のためのオペレーションズリサーチ工学博士栗原謙三工学博士明石吉三共著コロナ社\n",
      "Highest Similarity Ratio: 0.3714\n",
      "\n",
      "Text A (line 14): :0.41ネトハッカーズマニュアル西林小野史\n",
      "Best Match in Text B (line 16): キネクトハッカーズマニュアル西林孝＋小林憲史著Rutles\n",
      "Highest Similarity Ratio: 0.6415\n",
      "\n",
      "Text A (line 15): 生物の力学系ポール・ウォルトン\n",
      "Best Match in Text B (line 17): 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Similarity Ratio: 0.4638\n",
      "\n",
      "Text A (line 16): 40RobertFaludiMake:PROJECTSXBeeで作るワイヤレスセンサーネットワークKORELLY\n",
      "Best Match in Text B (line 18): Make:PROJECTSXBeeで作るワイヤレスセンサーネットワークRobertFaludi著小林茂監訳水原文訳O'REILLY\n",
      "Highest Similarity Ratio: 0.6829\n",
      "\n",
      "Text A (line 17): ZigBeeWi-FiBluetoothESEArduinoプログラム全集野CQ出版\n",
      "Best Match in Text B (line 19): ZigBee/Wi-Fi/Bluetooth無線用Arduinoプログラム全集国野亘著CD-ROM付きCQ出版社\n",
      "Highest Similarity Ratio: 0.8000\n",
      "\n",
      "Text A (line 18): デクステリティ巧みさとそのニコライ・ベルンシュタインKSTRE\n",
      "Best Match in Text B (line 20): デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Similarity Ratio: 0.6750\n",
      "\n",
      "Text A (line 19): 学理科系の作文技術木下R\n",
      "Best Match in Text B (line 21): 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Similarity Ratio: 0.6061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "        best_match_text_b = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = difflib.SequenceMatcher(None, text_a, text_b).ratio()\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "                best_match_text_b = text_b\n",
    "\n",
    "        print(f\"Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match in Text B (line {lines_b.index(best_match) + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e727c425-5ef0-4898-a436-836db1771d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"output_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = difflib.SequenceMatcher(None, text_a, text_b).ratio()\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Similarity Ratio: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7afaa8-03cb-41ba-9122-46a2d0c15d05",
   "metadata": {},
   "source": [
    "### Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b026ed89-2870-4469-9be3-05b8818d9871",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text A (line 1): 機械振動学湖著\n",
      "Best Match for Text A (line 1): 機械振動学佐藤秀紀岡部佐規一共著岩田佳雄元工業調査会\n",
      "Highest Similarity Ratio: 0.3636\n",
      "\n",
      "Original Text A (line 2): ロボット制御から動力学までロボット制御基礎から動力学まで浅ロボット制御\n",
      "Best Match for Text A (line 2): 実践ロボット制御基礎から動力学まで細田耕[著]HosodaKoh株式会社アールティ[協力]RTCorporationOhmsha\n",
      "Highest Similarity Ratio: 0.3030\n",
      "\n",
      "Original Text A (line 3): book情報理論と符号理論Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEchoGoogleHuneジョーンズポンダッドw.マイナビ\n",
      "Best Match for Text A (line 3): Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest Similarity Ratio: 0.8000\n",
      "\n",
      "Original Text A (line 4): P情報理論と符号理論ジョーンズTMジョーンズ\n",
      "Best Match for Text A (line 4): 情報理論と符合理論G・A・ジョーンズ著J・M・ジョーンズ一樂重雄/川原正治/川原雅子訳丸善\n",
      "Highest Similarity Ratio: 0.5672\n",
      "\n",
      "Original Text A (line 5): 29パターン認識と機械学習上CM・ビショップベイズによる\n",
      "Best Match for Text A (line 5): パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Similarity Ratio: 0.4651\n",
      "\n",
      "Original Text A (line 6): CM・ビショップパターン認識と機械学習1:3\n",
      "Best Match for Text A (line 6): 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest Similarity Ratio: 0.2951\n",
      "\n",
      "Original Text A (line 7): パターン認識と機械学習による\n",
      "Best Match for Text A (line 7): パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Similarity Ratio: 0.3889\n",
      "\n",
      "Original Text A (line 8): マルコフ連鎖から格子確率モデルへしCM・ビショップ\n",
      "Best Match for Text A (line 8): マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Similarity Ratio: 0.5294\n",
      "\n",
      "Original Text A (line 9): マルコフ連鎖から格子確率モデルへ\n",
      "Best Match for Text A (line 9): マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Similarity Ratio: 0.5424\n",
      "\n",
      "Original Text A (line 10): スパース性に基づく機械学習新同亮太上\n",
      "Best Match for Text A (line 10): 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest Similarity Ratio: 0.5263\n",
      "\n",
      "Original Text A (line 11): モジュラ化\n",
      "Best Match for Text A (line 11): MLPP機械学習シリーズプロフェッショナル劣モジュラ最適化と機械学習河原吉伸永野清仁講談社講談社\n",
      "Highest Similarity Ratio: 0.1887\n",
      "\n",
      "Original Text A (line 12): a\n",
      "Best Match for Text A (line 12): Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest Similarity Ratio: 0.0312\n",
      "\n",
      "Original Text A (line 13): オペレーションズリサーチ\n",
      "Best Match for Text A (line 13): 情報・技術経営シリーズ4経営情報処理のためのオペレーションズリサーチ工学博士栗原謙三工学博士明石吉三共著コロナ社\n",
      "Highest Similarity Ratio: 0.3529\n",
      "\n",
      "Original Text A (line 14): :0.41ネトハッカーズマニュアル西林小野史\n",
      "Best Match for Text A (line 14): キネクトハッカーズマニュアル西林孝＋小林憲史著Rutles\n",
      "Highest Similarity Ratio: 0.6275\n",
      "\n",
      "Original Text A (line 15): 生物の力学系ポール・ウォルトン\n",
      "Best Match for Text A (line 15): 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Similarity Ratio: 0.4478\n",
      "\n",
      "Original Text A (line 16): 40RobertFaludiMake:PROJECTSXBeeで作るワイヤレスセンサーネットワークKORELLY\n",
      "Best Match for Text A (line 16): Make:PROJECTSXBeeで作るワイヤレスセンサーネットワークRobertFaludi著小林茂監訳水原文訳O'REILLY\n",
      "Highest Similarity Ratio: 0.6777\n",
      "\n",
      "Original Text A (line 17): ZigBeeWi-FiBluetoothESEArduinoプログラム全集野CQ出版\n",
      "Best Match for Text A (line 17): ZigBee/Wi-Fi/Bluetooth無線用Arduinoプログラム全集国野亘著CD-ROM付きCQ出版社\n",
      "Highest Similarity Ratio: 0.7959\n",
      "\n",
      "Original Text A (line 18): デクステリティ巧みさとそのニコライ・ベルンシュタインKSTRE\n",
      "Best Match for Text A (line 18): デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Similarity Ratio: 0.6667\n",
      "\n",
      "Original Text A (line 19): 学理科系の作文技術木下R\n",
      "Best Match for Text A (line 19): 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Similarity Ratio: 0.6250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = Levenshtein.ratio(text_a.strip(), text_b.strip())\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "\n",
    "        print(f\"Original Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9b1a02a-d969-41cc-9259-99c68cd0197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"levenshtein_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = Levenshtein.ratio(text_a.strip(), text_b.strip())\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Similarity Ratio: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = text_b\n",
    "\n",
    "            output.write(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\\n\")\n",
    "            output.write(f\"Highest Similarity Ratio: {max_similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10ac69-6023-489f-b011-da3a9cdf0fbb",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ed485-78fd-493d-9a4e-3f0d31820c9a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "def jaccard_coefficient(s1, s2):\n",
    "    set1 = set(s1)\n",
    "    set2 = set(s2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    for index_a, text_a in enumerate(lines_a):\n",
    "        max_similarity = 0.0\n",
    "        best_match = None\n",
    "\n",
    "        for index_b, text_b in enumerate(lines_b):\n",
    "            similarity = jaccard_coefficient(text_a.strip(), text_b.strip())\n",
    "\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_match = text_b\n",
    "\n",
    "        print(f\"Original Text A (line {index_a + 1}): {text_a.strip()}\")\n",
    "        print(f\"Best Match for Text A (line {index_a + 1}): {best_match.strip()}\")\n",
    "        print(f\"Highest Jaccard Coefficient: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "416f1222-e91f-434b-bea1-228250c67966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(s1, s2):\n",
    "    set1 = set(s1)\n",
    "    set2 = set(s2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "    output_file = \"jaccard_similarity.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for index_a, text_a in enumerate(lines_a):\n",
    "            for index_b, text_b in enumerate(lines_b):\n",
    "                similarity = jaccard_coefficient(text_a.strip(), text_b.strip())\n",
    "\n",
    "                output.write(f\"Text A (line {index_a + 1}): {text_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {index_b + 1}): {text_b.strip()}\\n\")\n",
    "                output.write(f\"Jaccard Coefficient: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea725d-41c1-453f-abb7-7fa902f81cd5",
   "metadata": {},
   "source": [
    "### Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "351d00e9-c464-4b32-b490-7686a99c3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_cosine_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"cosine_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, line_b in enumerate(lines_b):\n",
    "                similarity = cosine_sim[i][j]\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {line_b.strip()}\\n\")\n",
    "                output.write(f\"Cosine Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f35f20-4530-4ddb-a9b7-9f8c29dd5957",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "efa820ea-4667-433a-b157-d3bb87a21d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全文出力\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_ngram_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_ngram_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"ngram_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"N-gram Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d33791a5-ee62-4a44-a4d8-421ffc03030d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 機械振動学湖著\n",
      "Best Match for Text A: 機械振動学佐藤秀紀岡部佐規一共著岩田佳雄元工業調査会\n",
      "Highest N-gram Similarity: 0.2449\n",
      "\n",
      "Text A (line 2): ロボット制御から動力学までロボット制御基礎から動力学まで浅ロボット制御\n",
      "Best Match for Text A: 実践ロボット制御基礎から動力学まで細田耕[著]HosodaKoh株式会社アールティ[協力]RTCorporationOhmsha\n",
      "Highest N-gram Similarity: 0.3972\n",
      "\n",
      "Text A (line 3): book情報理論と符号理論Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEchoGoogleHuneジョーンズポンダッドw.マイナビ\n",
      "Best Match for Text A: Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest N-gram Similarity: 0.7138\n",
      "\n",
      "Text A (line 4): P情報理論と符号理論ジョーンズTMジョーンズ\n",
      "Best Match for Text A: 情報理論と符合理論G・A・ジョーンズ著J・M・ジョーンズ一樂重雄/川原正治/川原雅子訳丸善\n",
      "Highest N-gram Similarity: 0.4190\n",
      "\n",
      "Text A (line 5): 29パターン認識と機械学習上CM・ビショップベイズによる\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest N-gram Similarity: 0.4333\n",
      "\n",
      "Text A (line 6): CM・ビショップパターン認識と機械学習1:3\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest N-gram Similarity: 0.4047\n",
      "\n",
      "Text A (line 7): パターン認識と機械学習による\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest N-gram Similarity: 0.3674\n",
      "\n",
      "Text A (line 8): マルコフ連鎖から格子確率モデルへしCM・ビショップ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest N-gram Similarity: 0.4410\n",
      "\n",
      "Text A (line 9): マルコフ連鎖から格子確率モデルへ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest N-gram Similarity: 0.5578\n",
      "\n",
      "Text A (line 10): スパース性に基づく機械学習新同亮太上\n",
      "Best Match for Text A: 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest N-gram Similarity: 0.4328\n",
      "\n",
      "Text A (line 11): モジュラ化\n",
      "Best Match for Text A: MLPP機械学習シリーズプロフェッショナル劣モジュラ最適化と機械学習河原吉伸永野清仁講談社講談社\n",
      "Highest N-gram Similarity: 0.1374\n",
      "\n",
      "Text A (line 13): オペレーションズリサーチ\n",
      "Best Match for Text A: 情報・技術経営シリーズ4経営情報処理のためのオペレーションズリサーチ工学博士栗原謙三工学博士明石吉三共著コロナ社\n",
      "Highest N-gram Similarity: 0.3925\n",
      "\n",
      "Text A (line 14): :0.41ネトハッカーズマニュアル西林小野史\n",
      "Best Match for Text A: キネクトハッカーズマニュアル西林孝＋小林憲史著Rutles\n",
      "Highest N-gram Similarity: 0.4536\n",
      "\n",
      "Text A (line 15): 生物の力学系ポール・ウォルトン\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest N-gram Similarity: 0.3742\n",
      "\n",
      "Text A (line 16): 40RobertFaludiMake:PROJECTSXBeeで作るワイヤレスセンサーネットワークKORELLY\n",
      "Best Match for Text A: Make:PROJECTSXBeeで作るワイヤレスセンサーネットワークRobertFaludi著小林茂監訳水原文訳O'REILLY\n",
      "Highest N-gram Similarity: 0.7585\n",
      "\n",
      "Text A (line 17): ZigBeeWi-FiBluetoothESEArduinoプログラム全集野CQ出版\n",
      "Best Match for Text A: ZigBee/Wi-Fi/Bluetooth無線用Arduinoプログラム全集国野亘著CD-ROM付きCQ出版社\n",
      "Highest N-gram Similarity: 0.5896\n",
      "\n",
      "Text A (line 18): デクステリティ巧みさとそのニコライ・ベルンシュタインKSTRE\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest N-gram Similarity: 0.5653\n",
      "\n",
      "Text A (line 19): 学理科系の作文技術木下R\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest N-gram Similarity: 0.5685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#最大値出力\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_ngram_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_ngram_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"ngram_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest N-gram Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest N-gram Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ec326-d1f4-45eb-88fa-1766cdfa17c7",
   "metadata": {},
   "source": [
    "### 部分文字列比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa8ac942-a15b-4e94-a266-f7002e8ed523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_partial_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_partial_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"partial_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"Partial Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d8c962b2-846e-4892-b4a3-d9b247cf6e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 機械振動学湖著\n",
      "Best Match for Text A: 機械振動学佐藤秀紀岡部佐規一共著岩田佳雄元工業調査会\n",
      "Highest Partial Similarity: 0.2449\n",
      "\n",
      "Text A (line 2): ロボット制御から動力学までロボット制御基礎から動力学まで浅ロボット制御\n",
      "Best Match for Text A: 実践ロボット制御基礎から動力学まで細田耕[著]HosodaKoh株式会社アールティ[協力]RTCorporationOhmsha\n",
      "Highest Partial Similarity: 0.3972\n",
      "\n",
      "Text A (line 3): book情報理論と符号理論Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEchoGoogleHuneジョーンズポンダッドw.マイナビ\n",
      "Best Match for Text A: Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest Partial Similarity: 0.7138\n",
      "\n",
      "Text A (line 4): P情報理論と符号理論ジョーンズTMジョーンズ\n",
      "Best Match for Text A: 情報理論と符合理論G・A・ジョーンズ著J・M・ジョーンズ一樂重雄/川原正治/川原雅子訳丸善\n",
      "Highest Partial Similarity: 0.4190\n",
      "\n",
      "Text A (line 5): 29パターン認識と機械学習上CM・ビショップベイズによる\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Partial Similarity: 0.4333\n",
      "\n",
      "Text A (line 6): CM・ビショップパターン認識と機械学習1:3\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Partial Similarity: 0.4047\n",
      "\n",
      "Text A (line 7): パターン認識と機械学習による\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Partial Similarity: 0.3674\n",
      "\n",
      "Text A (line 8): マルコフ連鎖から格子確率モデルへしCM・ビショップ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Partial Similarity: 0.4410\n",
      "\n",
      "Text A (line 9): マルコフ連鎖から格子確率モデルへ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Partial Similarity: 0.5578\n",
      "\n",
      "Text A (line 10): スパース性に基づく機械学習新同亮太上\n",
      "Best Match for Text A: 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest Partial Similarity: 0.4328\n",
      "\n",
      "Text A (line 11): モジュラ化\n",
      "Best Match for Text A: MLPP機械学習シリーズプロフェッショナル劣モジュラ最適化と機械学習河原吉伸永野清仁講談社講談社\n",
      "Highest Partial Similarity: 0.1374\n",
      "\n",
      "Text A (line 13): オペレーションズリサーチ\n",
      "Best Match for Text A: 情報・技術経営シリーズ4経営情報処理のためのオペレーションズリサーチ工学博士栗原謙三工学博士明石吉三共著コロナ社\n",
      "Highest Partial Similarity: 0.3925\n",
      "\n",
      "Text A (line 14): :0.41ネトハッカーズマニュアル西林小野史\n",
      "Best Match for Text A: キネクトハッカーズマニュアル西林孝＋小林憲史著Rutles\n",
      "Highest Partial Similarity: 0.4536\n",
      "\n",
      "Text A (line 15): 生物の力学系ポール・ウォルトン\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Partial Similarity: 0.3742\n",
      "\n",
      "Text A (line 16): 40RobertFaludiMake:PROJECTSXBeeで作るワイヤレスセンサーネットワークKORELLY\n",
      "Best Match for Text A: Make:PROJECTSXBeeで作るワイヤレスセンサーネットワークRobertFaludi著小林茂監訳水原文訳O'REILLY\n",
      "Highest Partial Similarity: 0.7585\n",
      "\n",
      "Text A (line 17): ZigBeeWi-FiBluetoothESEArduinoプログラム全集野CQ出版\n",
      "Best Match for Text A: ZigBee/Wi-Fi/Bluetooth無線用Arduinoプログラム全集国野亘著CD-ROM付きCQ出版社\n",
      "Highest Partial Similarity: 0.5896\n",
      "\n",
      "Text A (line 18): デクステリティ巧みさとそのニコライ・ベルンシュタインKSTRE\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Partial Similarity: 0.5653\n",
      "\n",
      "Text A (line 19): 学理科系の作文技術木下R\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Partial Similarity: 0.5685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_partial_similarity(texts):\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    return cosine_sim\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    cosine_sim = calculate_partial_similarity(lines_a + lines_b)\n",
    "\n",
    "    output_file = \"partial_similarity_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, similarity in enumerate(cosine_sim[i][len(lines_a):]):\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest Partial Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest Partial Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54edd8f-1b93-4940-9d31-fc4f5efcd12d",
   "metadata": {},
   "source": [
    "### ローマッチング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "77f27daa-d48a-4e6b-8dbb-ac1d33133626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def calculate_levenshtein_distance(texts):\n",
    "    distance_matrix = [[0 for _ in range(len(texts[1]))] for _ in range(len(texts[0]))]\n",
    "    for i in range(len(texts[0])):\n",
    "        for j in range(len(texts[1])):\n",
    "            distance_matrix[i][j] = Levenshtein.distance(texts[0][i], texts[1][j])\n",
    "    return distance_matrix\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    levenshtein_distances = calculate_levenshtein_distance([lines_a, lines_b])\n",
    "\n",
    "    output_file = \"levenshtein_distance_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            for j, distance in enumerate(levenshtein_distances[i]):\n",
    "                similarity = 1 - (distance / max(len(lines_a[i]), len(lines_b[j])))\n",
    "                output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "                output.write(f\"Text B (line {j + 1}): {lines_b[j].strip()}\\n\")\n",
    "                output.write(f\"Levenshtein Similarity: {similarity:.4f}\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "538f1bff-c0eb-47e7-bbd4-76e38cd92d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text A (line 1): 機械振動学湖著\n",
      "Best Match for Text A: 機械振動学佐藤秀紀岡部佐規一共著岩田佳雄元工業調査会\n",
      "Highest Levenshtein Similarity: 0.2593\n",
      "\n",
      "Text A (line 2): ロボット制御から動力学までロボット制御基礎から動力学まで浅ロボット制御\n",
      "Best Match for Text A: 実践ロボット制御基礎から動力学まで細田耕[著]HosodaKoh株式会社アールティ[協力]RTCorporationOhmsha\n",
      "Highest Levenshtein Similarity: 0.2308\n",
      "\n",
      "Text A (line 3): book情報理論と符号理論Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEchoGoogleHuneジョーンズポンダッドw.マイナビ\n",
      "Best Match for Text A: Aプログラミングスマートスピーカー×自分でつくる人工知能AmazonEcho.GoogleHomeポンダッド[著]W.マイナビ\n",
      "Highest Levenshtein Similarity: 0.6795\n",
      "\n",
      "Text A (line 4): P情報理論と符号理論ジョーンズTMジョーンズ\n",
      "Best Match for Text A: 情報理論と符合理論G・A・ジョーンズ著J・M・ジョーンズ一樂重雄/川原正治/川原雅子訳丸善\n",
      "Highest Levenshtein Similarity: 0.4130\n",
      "\n",
      "Text A (line 5): 29パターン認識と機械学習上CM・ビショップベイズによる\n",
      "Best Match for Text A: パターン認識と機械学習上ベイズ理論による統計的予測C・M・ビショップ著元田浩栗田多喜夫樋口知之松本裕治村田昇監訳丸善\n",
      "Highest Levenshtein Similarity: 0.3220\n",
      "\n",
      "Text A (line 6): CM・ビショップパターン認識と機械学習1:3\n",
      "Best Match for Text A: はじめてのパターン認識平井有三著8497\n",
      "Highest Levenshtein Similarity: 0.2609\n",
      "\n",
      "Text A (line 7): パターン認識と機械学習による\n",
      "Best Match for Text A: はじめてのパターン認識平井有三著8497\n",
      "Highest Levenshtein Similarity: 0.3333\n",
      "\n",
      "Text A (line 8): マルコフ連鎖から格子確率モデルへしCM・ビショップ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Levenshtein Similarity: 0.4091\n",
      "\n",
      "Text A (line 9): マルコフ連鎖から格子確率モデルへ\n",
      "Best Match for Text A: マルコフ連鎖から格子確率モデルへR・B・シナジ著現代確率論の基礎と応用今野紀雄林俊一訳\n",
      "Highest Levenshtein Similarity: 0.3864\n",
      "\n",
      "Text A (line 10): スパース性に基づく機械学習新同亮太上\n",
      "Best Match for Text A: 2学習MLPシリーズプロフェッショナルスパース性に基づく機械学習冨岡亮太講談社\n",
      "Highest Levenshtein Similarity: 0.4000\n",
      "\n",
      "Text A (line 11): モジュラ化\n",
      "Best Match for Text A: MLPP機械学習シリーズプロフェッショナル劣モジュラ最適化と機械学習河原吉伸永野清仁講談社講談社\n",
      "Highest Levenshtein Similarity: 0.1224\n",
      "\n",
      "Text A (line 12): a\n",
      "Best Match for Text A: はじめてのパターン認識平井有三著8497\n",
      "Highest Levenshtein Similarity: 0.0476\n",
      "\n",
      "Text A (line 13): オペレーションズリサーチ\n",
      "Best Match for Text A: 情報・技術経営シリーズ4経営情報処理のためのオペレーションズリサーチ工学博士栗原謙三工学博士明石吉三共著コロナ社\n",
      "Highest Levenshtein Similarity: 0.2281\n",
      "\n",
      "Text A (line 14): :0.41ネトハッカーズマニュアル西林小野史\n",
      "Best Match for Text A: キネクトハッカーズマニュアル西林孝＋小林憲史著Rutles\n",
      "Highest Levenshtein Similarity: 0.4333\n",
      "\n",
      "Text A (line 15): 生物の力学系ポール・ウォルトン\n",
      "Best Match for Text A: 微生物の力学系ケモスタット理論を通してハル・スミス＆ポール・ウォルトマン[著]竹内康博[監訳]日本評議院\n",
      "Highest Levenshtein Similarity: 0.3019\n",
      "\n",
      "Text A (line 16): 40RobertFaludiMake:PROJECTSXBeeで作るワイヤレスセンサーネットワークKORELLY\n",
      "Best Match for Text A: Make:PROJECTSXBeeで作るワイヤレスセンサーネットワークRobertFaludi著小林茂監訳水原文訳O'REILLY\n",
      "Highest Levenshtein Similarity: 0.4242\n",
      "\n",
      "Text A (line 17): ZigBeeWi-FiBluetoothESEArduinoプログラム全集野CQ出版\n",
      "Best Match for Text A: ZigBee/Wi-Fi/Bluetooth無線用Arduinoプログラム全集国野亘著CD-ROM付きCQ出版社\n",
      "Highest Levenshtein Similarity: 0.7018\n",
      "\n",
      "Text A (line 18): デクステリティ巧みさとそのニコライ・ベルンシュタインKSTRE\n",
      "Best Match for Text A: デクステリティ巧みさとその発達ニコライ・A・ベルンシュタイン著工藤和俊訳佐々木正人監訳金子書房\n",
      "Highest Levenshtein Similarity: 0.5625\n",
      "\n",
      "Text A (line 19): 学理科系の作文技術木下R\n",
      "Best Match for Text A: 理科系の作文技術木下是雄著中公新書624\n",
      "Highest Levenshtein Similarity: 0.4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def calculate_levenshtein_distance(texts):\n",
    "    distance_matrix = [[0 for _ in range(len(texts[1]))] for _ in range(len(texts[0]))]\n",
    "    for i in range(len(texts[0])):\n",
    "        for j in range(len(texts[1])):\n",
    "            distance_matrix[i][j] = Levenshtein.distance(texts[0][i], texts[1][j])\n",
    "    return distance_matrix\n",
    "\n",
    "def main():\n",
    "    file_a_path = \"output_results.txt\"\n",
    "    file_b_path = \"database.txt\"\n",
    "\n",
    "    with open(file_a_path, \"r\", encoding=\"utf-8\") as file_a:\n",
    "        lines_a = file_a.readlines()\n",
    "\n",
    "    with open(file_b_path, \"r\", encoding=\"utf-8\") as file_b:\n",
    "        lines_b = file_b.readlines()\n",
    "\n",
    "    levenshtein_distances = calculate_levenshtein_distance([lines_a, lines_b])\n",
    "\n",
    "    output_file = \"levenshtein_distance_results.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "        for i, line_a in enumerate(lines_a):\n",
    "            max_similarity = 0.0\n",
    "            best_match = None\n",
    "            best_match_index = -1\n",
    "            for j, distance in enumerate(levenshtein_distances[i]):\n",
    "                similarity = 1 - (distance / max(len(lines_a[i]), len(lines_b[j])))\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = lines_b[j].strip()\n",
    "                    best_match_index = j\n",
    "            output.write(f\"Text A (line {i + 1}): {line_a.strip()}\\n\")\n",
    "            output.write(f\"Best Match for Text A: {best_match}\\n\")\n",
    "            output.write(f\"Highest Levenshtein Similarity: {max_similarity:.4f}\\n\\n\")\n",
    "            if best_match_index != -1:\n",
    "                print(f\"Text A (line {i + 1}): {line_a.strip()}\")\n",
    "                print(f\"Best Match for Text A: {lines_b[best_match_index].strip()}\")\n",
    "                print(f\"Highest Levenshtein Similarity: {max_similarity:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf852f-1299-46c6-b09a-765ee6fc57b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "localGPU",
   "language": "python",
   "name": "localgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
