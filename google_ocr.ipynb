{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf1c2e17-ea96-45fe-b65e-fb272c1f2851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac7c231-0c65-49b7-b238-500c06f76957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25f64ed-b8f0-4ef4-a3e3-e9d53f6c041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9553233397465277421\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 22722641920\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6692181266078560799\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8868f1b6-6dde-4ae1-8662-ae4790eb03bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0, cv2.CAP_MSMF)\n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "cap.set(cv2.CAP_PROP_FPS, 60)           # カメラFPSを60FPSに設定\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920) # カメラ画像の横幅を1280に設定\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080) # カメラ画像の縦幅を720に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6bfbe1c-2eda-472d-95e9-0001973a0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capturing\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "cv2.imwrite(\"C:/Users/covid/text_recognition/yolov7/input.png\", frame)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddd81928-34a6-4223-9e45-7c8283efac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covid\\text_recognition\\yolov7\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\\yolov7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c2ccd2-7059-4b24-967a-3b588652eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリのパス\n",
    "directory_path = \"C:/Users/covid/text_recognition/yolov7/runs/detect/\"\n",
    "\n",
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3504fcd-93e7-427a-944d-161d4f3b56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7-e6e.pt'], source='C:/Users/covid/text_recognition/yolov7/input.png', img_size=1280, conf_thres=0.25, iou_thres=0.45, device='0', view_img=False, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  v0.1-126-g84932d7 torch 2.1.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3090, 24575.5MB)\n",
      "\n",
      "Model Summary: 792 layers, 151687420 parameters, 817020 gradients\n",
      "C:\\Users\\covid\\anaconda3\\envs\\localGPU\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\covid\\text_recognition\\yolov7\\detect.py\", line 196, in <module>\n",
      "    detect()\n",
      "  File \"C:\\Users\\covid\\text_recognition\\yolov7\\detect.py\", line 92, in detect\n",
      "    pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
      "  File \"C:\\Users\\covid\\text_recognition\\yolov7\\utils\\general.py\", line 684, in non_max_suppression\n",
      "    i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
      "  File \"C:\\Users\\covid\\anaconda3\\envs\\localGPU\\lib\\site-packages\\torchvision\\ops\\boxes.py\", line 41, in nms\n",
      "    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)\n",
      "  File \"C:\\Users\\covid\\anaconda3\\envs\\localGPU\\lib\\site-packages\\torch\\_ops.py\", line 692, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "CPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\n",
      "QuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\n",
      "BackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\n",
      "Functionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:290 [backend fallback]\n",
      "Named: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\n",
      "ZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\n",
      "AutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\n",
      "AutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\n",
      "AutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\n",
      "AutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\n",
      "AutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\n",
      "AutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\n",
      "AutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\n",
      "AutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\n",
      "AutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\n",
      "Tracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:296 [backend fallback]\n",
      "AutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:382 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:249 [backend fallback]\n",
      "FuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:710 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\n",
      "Batched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\n",
      "PythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\n",
      "PreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source C:/Users/covid/text_recognition/yolov7/input.png --weights yolov7-e6e.pt --conf 0.25 --img-size 1280 --device 0 --save-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1820bb9-6774-4ac4-8085-ab3c7f8ec3a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m image_absolute_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(newest_subdirectory_path, image_relative_path)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# テキストファイルの絶対パスを作成\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m text_file_absolute_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mcurrent_directory\u001b[49m, text_file_relative_path)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 画像をImageクラスのインスタンスに読み込む\u001b[39;00m\n\u001b[0;32m     27\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_absolute_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_directory' is not defined"
     ]
    }
   ],
   "source": [
    "# ディレクトリのパス\n",
    "directory_path = \"C:/Users/covid/text_recognition/yolov7/runs/detect/\"\n",
    "\n",
    "# ディレクトリ内のサブディレクトリのリストを取得\n",
    "subdirectories = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "# サブディレクトリの中で一番新しいものを取得\n",
    "newest_subdirectory = max(subdirectories, key=lambda d: os.path.getctime(os.path.join(directory_path, d)))\n",
    "\n",
    "# 最新のサブディレクトリのパスを作成\n",
    "newest_subdirectory_path = os.path.join(directory_path, newest_subdirectory)\n",
    "\n",
    "# 新しいディレクトリに移動\n",
    "os.chdir(newest_subdirectory_path)\n",
    "\n",
    "# 画像ファイルの相対パスを指定\n",
    "image_relative_path = \"input.png\"\n",
    "# テキストファイルの相対パスを指定\n",
    "text_file_relative_path = \"labels/input.txt\"\n",
    "\n",
    "# 画像ファイルの絶対パスを作成\n",
    "image_absolute_path = os.path.join(newest_subdirectory_path, image_relative_path)\n",
    "# テキストファイルの絶対パスを作成\n",
    "text_file_absolute_path = os.path.join(current_directory, text_file_relative_path)\n",
    "\n",
    "# 画像をImageクラスのインスタンスに読み込む\n",
    "image = Image.open(image_absolute_path)\n",
    "# テキストファイルを読み込む\n",
    "with open(text_file_absolute_path, 'r') as file:\n",
    "    text_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de5ad9-6704-4ed8-88cd-69cf289abd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wid,hei = image.size\n",
    "\n",
    "# 画像の表示とサイズの変更\n",
    "plt.figure(figsize=(image.width / 100, image.height / 100))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # 軸を表示しない\n",
    "\n",
    "plt.tight_layout()  # 表示領域を調整\n",
    "\n",
    "plt.show()  # 画像の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fd2d9-71f2-47b1-a602-2b1d9b7c14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(text_file):\n",
    "    output_dir = 'output'\n",
    "    if os.path.exists(output_dir):\n",
    "        file_list = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(output_dir, file_name)\n",
    "            os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        lines = sorted(lines, key=lambda line: float(line.split()[1]))\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            values = line.split()\n",
    "\n",
    "            if len(values) == 5:\n",
    "                object_class = values[0]\n",
    "                a = float(values[1])\n",
    "                b = float(values[2])\n",
    "                c = float(values[3])\n",
    "                d = float(values[4])\n",
    "\n",
    "                # Calculate coordinates and dimensions\n",
    "                x_center = int(wid * a)\n",
    "                y_center = int(hei * b)\n",
    "                width = int(wid * c)\n",
    "                height = int(hei * d)\n",
    "\n",
    "                x_min = x_center - width // 2\n",
    "                y_min = y_center - height // 2\n",
    "                x_max = x_center + width // 2\n",
    "                y_max = y_center + height // 2\n",
    "\n",
    "                output_filename = os.path.join(output_dir, f'book{i+1}.jpg')\n",
    "                index = 1\n",
    "                while os.path.exists(output_filename):\n",
    "                    output_filename = os.path.join(output_dir, f'book{i+1}_{index}.jpg')\n",
    "                    index += 1\n",
    "\n",
    "                # Crop and save the image\n",
    "                cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "                cropped.save(output_filename)\n",
    "\n",
    "# Process the text file\n",
    "process_text_file(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d76ea-5fcc-49fa-97d4-2e7f088dd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"C:\\Users\\covid\\text_recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c082f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T14:53:53.138703Z",
     "start_time": "2023-05-23T14:53:53.097171Z"
    }
   },
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "MIME_TYPE = 'application/vnd.google-apps.document'\n",
    "APPLICATION_NAME = 'ipa-google-drive-api-client'\n",
    "\n",
    "def get_service():\n",
    "\n",
    "    # credentialの取得\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'google-drive-api.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        \n",
    "    # serviceの取得\n",
    "    service = build('drive', 'v3', credentials=creds) \n",
    "    \n",
    "    return service\n",
    "\n",
    "def read_ocr(service, input_file, lang='en'):\n",
    "    # ファイルのアップロード\n",
    "\n",
    "    # ローカルファイルの定義\n",
    "    media_body = MediaFileUpload(input_file, mimetype=MIME_TYPE, resumable=True)\n",
    "\n",
    "    # Google Drive上のファイル名\n",
    "    newfile = 'output.pdf'\n",
    "\n",
    "    body = {\n",
    "        'name': newfile,\n",
    "        'mimeType': MIME_TYPE\n",
    "    }\n",
    "\n",
    "    # 　creat関数でファイルアップロード実行\n",
    "    # 同時にOCR読み取りも行う\n",
    "    output = service.files().create(\n",
    "        body=body,\n",
    "        media_body=media_body,\n",
    "        # ここで読み込み先言語の指定を行う\n",
    "        ocrLanguage=lang,\n",
    "    ).execute()\n",
    "\n",
    "    # テキストファイルのダウンロード\n",
    "\n",
    "    # リクエストオブジェクト生成\n",
    "    request = service.files().export_media(\n",
    "        fileId=output['id'],\n",
    "        mimeType=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    # 出力用テキストファイル名\n",
    "    output_path = 'output.txt'\n",
    "\n",
    "    with open(output_path, 'a') as f:\n",
    "        fh = io.FileIO(output_path, \"wb\")\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "\n",
    "        service.files().delete(fileId=output['id']).execute()\n",
    "\n",
    "        # テキストの取得\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 既存のファイルから読み取り結果を1文にまとめて追記する\n",
    "    output_text = ''.join([line.strip() for line in lines])\n",
    "\n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(output_text)\n",
    "\n",
    "    # 読み取り結果のリストを返す\n",
    "    return lines[1:]\n",
    "\n",
    "\n",
    "service = get_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e5781-5c29-47ee-84b7-2f6160a35a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output corresponding to list format\n",
    "if __name__ == '__main__':\n",
    "    directory_path = 'C:/Users/covid/text_recognition/yolov7/output'\n",
    "    output_list = []\n",
    "\n",
    "    file_list = [filename for filename in os.listdir(directory_path) if filename.endswith('.jpg')]\n",
    "    file_list.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "    for filename in file_list:\n",
    "        input_file = os.path.join(directory_path, filename)\n",
    "        output = read_ocr(service, input_file, 'ja')\n",
    "\n",
    "        # 不要な文字（スペースとバックスラッシュ）を除去して一つの文字列に結合する\n",
    "        cleaned_output = ''.join(line.strip().replace(' ', '').replace('/', '').replace('\\n', '').replace('\\\\', '') for line in output)\n",
    "\n",
    "        # 結果をリストに追加\n",
    "        output_list.append(cleaned_output)\n",
    "\n",
    "    # 結果のリストを出力\n",
    "    for result in output_list:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723e186-f9da-47ef-889f-f2c055312313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c0be0-0a01-4a69-9541-2646a25b55c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "localGPU",
   "language": "python",
   "name": "localgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
